{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ddavila9789/Data-Analysis-with-Spark/blob/main/Data_Analysis_with_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ZtN_7zPojhlo"
      },
      "outputs": [],
      "source": [
        "# Installing required packages\n",
        "\n",
        "!pip install pysparkâ€¯ findspark wget\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": [],
        "id": "G3xLX7CWjhlp"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": [],
        "id": "suEIsRWvjhlp"
      },
      "outputs": [],
      "source": [
        "# PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the SparkContext.\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": [],
        "id": "aFJsWl_Vjhlp"
      },
      "outputs": [],
      "source": [
        "# Creating a SparkContext object\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Creating a SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark DataFrames basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txBE8tddjhlq"
      },
      "source": [
        "2. Download the CSV data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-LC5nsOyjhlq",
        "outputId": "a374230c-7309-45a4-c3c4-face23d7c663"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'employees.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Download the CSV data first into a local `employees.csv` file\n",
        "import wget\n",
        "wget.download(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/data/employees.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E25ppzxhjhlq"
      },
      "source": [
        "### Tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gK3wnYfjhlq"
      },
      "source": [
        "#### Task 1: Generate a Spark DataFrame from the CSV data\n",
        "\n",
        "Read data from the provided CSV file, `employees.csv` and import it into a Spark DataFrame variable named `employees_df`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AIvvljXjhlq",
        "outputId": "c21b7c81-52ca-41ed-90ae-dc71e7e438a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+------+---+----------+\n",
            "|Emp_No| Emp_Name|Salary|Age|Department|\n",
            "+------+---------+------+---+----------+\n",
            "|   198|   Donald|  2600| 29|        IT|\n",
            "|   199|  Douglas|  2600| 34|     Sales|\n",
            "|   200| Jennifer|  4400| 36| Marketing|\n",
            "|   201|  Michael| 13000| 32|        IT|\n",
            "|   202|      Pat|  6000| 39|        HR|\n",
            "|   203|    Susan|  6500| 36| Marketing|\n",
            "|   204|  Hermann| 10000| 29|   Finance|\n",
            "|   205|  Shelley| 12008| 33|   Finance|\n",
            "|   206|  William|  8300| 37|        IT|\n",
            "|   100|   Steven| 24000| 39|        IT|\n",
            "|   101|    Neena| 17000| 27|     Sales|\n",
            "|   102|      Lex| 17000| 37| Marketing|\n",
            "|   103|Alexander|  9000| 39| Marketing|\n",
            "|   104|    Bruce|  6000| 38|        IT|\n",
            "|   105|    David|  4800| 39|        IT|\n",
            "|   106|    Valli|  4800| 38|     Sales|\n",
            "|   107|    Diana|  4200| 35|     Sales|\n",
            "|   108|    Nancy| 12008| 28|     Sales|\n",
            "|   109|   Daniel|  9000| 35|        HR|\n",
            "|   110|     John|  8200| 31| Marketing|\n",
            "+------+---------+------+---+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read data from the \"emp\" CSV file and import it into a DataFrame variable named \"employees_df\"\n",
        "employees_df = spark.read.csv(\"employees.csv\", header=True)\n",
        "\n",
        "employees_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zfj0tjWVjhlr"
      },
      "source": [
        "#### Task 2: Define a schema for the data\n",
        "\n",
        "Construct a schema for the input data and then utilize the defined schema to read the CSV file to create a DataFrame named `employees_df`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "EHLBwwqEjhlr",
        "outputId": "84b83d33-d85f-4d15-fc4d-0d9b309f8668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+---------+------+---+----------+\n",
            "|Emp_No| Emp_Name|Salary|Age|Department|\n",
            "+------+---------+------+---+----------+\n",
            "|   198|   Donald|  2600| 29|        IT|\n",
            "|   199|  Douglas|  2600| 34|     Sales|\n",
            "|   200| Jennifer|  4400| 36| Marketing|\n",
            "|   201|  Michael| 13000| 32|        IT|\n",
            "|   202|      Pat|  6000| 39|        HR|\n",
            "|   203|    Susan|  6500| 36| Marketing|\n",
            "|   204|  Hermann| 10000| 29|   Finance|\n",
            "|   205|  Shelley| 12008| 33|   Finance|\n",
            "|   206|  William|  8300| 37|        IT|\n",
            "|   100|   Steven| 24000| 39|        IT|\n",
            "|   101|    Neena| 17000| 27|     Sales|\n",
            "|   102|      Lex| 17000| 37| Marketing|\n",
            "|   103|Alexander|  9000| 39| Marketing|\n",
            "|   104|    Bruce|  6000| 38|        IT|\n",
            "|   105|    David|  4800| 39|        IT|\n",
            "|   106|    Valli|  4800| 38|     Sales|\n",
            "|   107|    Diana|  4200| 35|     Sales|\n",
            "|   108|    Nancy| 12008| 28|     Sales|\n",
            "|   109|   Daniel|  9000| 35|        HR|\n",
            "|   110|     John|  8200| 31| Marketing|\n",
            "+------+---------+------+---+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define a Schema for the input data and read the file using the user-defined Schema\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "employees_df = employees_df.withColumn(\"Emp_No\", employees_df[\"Emp_No\"].cast(IntegerType()))\n",
        "employees_df = employees_df.withColumn(\"Salary\", employees_df[\"Salary\"].cast(IntegerType()))\n",
        "\n",
        "new_schema = StructType([\n",
        "    StructField(\"Emp_No\", IntegerType(), nullable=False),\n",
        "    StructField(\"Emp_Name\", StringType(), nullable=False),\n",
        "    StructField(\"Salary\", IntegerType(), nullable=False),\n",
        "    StructField(\"Age\", StringType(), nullable=False),\n",
        "    StructField(\"Department\", StringType(), nullable=False)\n",
        "])\n",
        "\n",
        "# Apply the new schema to the existing DataFrame\n",
        "employees_df = spark.createDataFrame(employees_df.rdd, schema=new_schema)\n",
        "\n",
        "# Show the DataFrame with the new schema\n",
        "employees_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAsFxGTWjhlr"
      },
      "source": [
        "#### Task 3: Display schema of DataFrame\n",
        "\n",
        "Display the schema of the `employees_df` DataFrame, showing all columns and their respective data types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "bYw8C4g2jhlr",
        "outputId": "e72ac9f7-ca0b-4862-b1a0-0c9193f226a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Emp_No: integer (nullable = false)\n",
            " |-- Emp_Name: string (nullable = false)\n",
            " |-- Salary: integer (nullable = false)\n",
            " |-- Age: string (nullable = false)\n",
            " |-- Department: string (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display all columns of the DataFrame, along with their respective data types\n",
        "employees_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTQvIAAxjhlr"
      },
      "source": [
        "#### Task 4: Create a temporary view\n",
        "\n",
        "Create a temporary view named `employees` for the `employees_df` DataFrame, enabling Spark SQL queries on the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Nyybe2Q1jhlr"
      },
      "outputs": [],
      "source": [
        "# Create a temporary view named \"employees\" for the DataFrame\n",
        "employees_df.createOrReplaceTempView('employees')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDhRJcsGjhlr"
      },
      "source": [
        "#### Task 5: Execute an SQL query\n",
        "\n",
        "Compose and execute an SQL query to fetch the records from the `employees` view where the age of employees exceeds 30. Then, display the result of the SQL query, showcasing the filtered records.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "1XgHXfwcjhlr",
        "outputId": "81c1732a-1b12-4a16-ea13-41ee543d5f3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-----------+------+---+----------+\n",
            "|Emp_No|   Emp_Name|Salary|Age|Department|\n",
            "+------+-----------+------+---+----------+\n",
            "|   199|    Douglas|  2600| 34|     Sales|\n",
            "|   200|   Jennifer|  4400| 36| Marketing|\n",
            "|   201|    Michael| 13000| 32|        IT|\n",
            "|   202|        Pat|  6000| 39|        HR|\n",
            "|   203|      Susan|  6500| 36| Marketing|\n",
            "|   205|    Shelley| 12008| 33|   Finance|\n",
            "|   206|    William|  8300| 37|        IT|\n",
            "|   100|     Steven| 24000| 39|        IT|\n",
            "|   102|        Lex| 17000| 37| Marketing|\n",
            "|   103|  Alexander|  9000| 39| Marketing|\n",
            "|   104|      Bruce|  6000| 38|        IT|\n",
            "|   105|      David|  4800| 39|        IT|\n",
            "|   106|      Valli|  4800| 38|     Sales|\n",
            "|   107|      Diana|  4200| 35|     Sales|\n",
            "|   109|     Daniel|  9000| 35|        HR|\n",
            "|   110|       John|  8200| 31| Marketing|\n",
            "|   111|     Ismael|  7700| 32|        IT|\n",
            "|   112|Jose Manuel|  7800| 34|        HR|\n",
            "|   113|       Luis|  6900| 34|     Sales|\n",
            "|   116|     Shelli|  2900| 37|   Finance|\n",
            "+------+-----------+------+---+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# SQL query to fetch solely the records from the View where the age exceeds 30\n",
        "result = spark.sql(\"SELECT * FROM employees WHERE age > 30\")\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NEzHsfgjhlr"
      },
      "source": [
        "#### Task 6: Calculate Average Salary by Department\n",
        "\n",
        "Compose an SQL query to retrieve the average salary of employees grouped by department. Display the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "eGAo4WUJjhls",
        "outputId": "8964cd6f-8adc-47bd-fa70-8f0f8a4a481a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------------+\n",
            "|Department|average_Salary|\n",
            "+----------+--------------+\n",
            "|     Sales|        5493.0|\n",
            "|        HR|        5838.0|\n",
            "|   Finance|        5731.0|\n",
            "| Marketing|        6633.0|\n",
            "|        IT|        7400.0|\n",
            "+----------+--------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# SQL query to calculate the average salary of employees grouped by department\n",
        "result = spark.sql(\"SELECT Department, ROUND(AVG(Salary)) as average_Salary FROM employees GROUP BY Department\")\n",
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oPVa_hsjhls"
      },
      "source": [
        "#### Task 7: Filter and Display IT Department Employees\n",
        "\n",
        "Apply a filter on the `employees_df` DataFrame to select records where the department is `'IT'`. Display the filtered DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "gLFFCH54jhls",
        "outputId": "a4605002-1f9b-46b9-e2a1-a0cc90e5de10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------+------+---+----------+\n",
            "|Emp_No|Emp_Name|Salary|Age|Department|\n",
            "+------+--------+------+---+----------+\n",
            "|   198|  Donald|  2600| 29|        IT|\n",
            "|   201| Michael| 13000| 32|        IT|\n",
            "|   206| William|  8300| 37|        IT|\n",
            "|   100|  Steven| 24000| 39|        IT|\n",
            "|   104|   Bruce|  6000| 38|        IT|\n",
            "|   105|   David|  4800| 39|        IT|\n",
            "|   111|  Ismael|  7700| 32|        IT|\n",
            "|   129|   Laura|  3300| 38|        IT|\n",
            "|   132|      TJ|  2100| 34|        IT|\n",
            "|   136|   Hazel|  2200| 29|        IT|\n",
            "+------+--------+------+---+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Apply a filter to select records where the department is 'IT'\n",
        "filtered_df = spark.sql(\"SELECT * FROM employees WHERE Department ='IT'\")\n",
        "filtered_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jeAQubyjhls"
      },
      "source": [
        "#### Task 8: Add 10% Bonus to Salaries\n",
        "\n",
        "Perform a transformation to add a new column named \"SalaryAfterBonus\" to the DataFrame. Calculate the new salary by adding a 10% bonus to each employee's salary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "SeUK3TT0jhls",
        "outputId": "8baf2f29-6086-4b68-f130-236a13598a29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+---------+------+---+----------+----------------+\n",
            "|Emp_No| Emp_Name|Salary|Age|Department|SalaryAfterBonus|\n",
            "+------+---------+------+---+----------+----------------+\n",
            "|   198|   Donald|  2600| 29|        IT|          2860.0|\n",
            "|   199|  Douglas|  2600| 34|     Sales|          2860.0|\n",
            "|   200| Jennifer|  4400| 36| Marketing|          4840.0|\n",
            "|   201|  Michael| 13000| 32|        IT|         14300.0|\n",
            "|   202|      Pat|  6000| 39|        HR|          6600.0|\n",
            "|   203|    Susan|  6500| 36| Marketing|          7150.0|\n",
            "|   204|  Hermann| 10000| 29|   Finance|         11000.0|\n",
            "|   205|  Shelley| 12008| 33|   Finance|         13209.0|\n",
            "|   206|  William|  8300| 37|        IT|          9130.0|\n",
            "|   100|   Steven| 24000| 39|        IT|         26400.0|\n",
            "|   101|    Neena| 17000| 27|     Sales|         18700.0|\n",
            "|   102|      Lex| 17000| 37| Marketing|         18700.0|\n",
            "|   103|Alexander|  9000| 39| Marketing|          9900.0|\n",
            "|   104|    Bruce|  6000| 38|        IT|          6600.0|\n",
            "|   105|    David|  4800| 39|        IT|          5280.0|\n",
            "|   106|    Valli|  4800| 38|     Sales|          5280.0|\n",
            "|   107|    Diana|  4200| 35|     Sales|          4620.0|\n",
            "|   108|    Nancy| 12008| 28|     Sales|         13209.0|\n",
            "|   109|   Daniel|  9000| 35|        HR|          9900.0|\n",
            "|   110|     John|  8200| 31| Marketing|          9020.0|\n",
            "+------+---------+------+---+----------+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, round\n",
        "\n",
        "# Add a new column \"SalaryAfterBonus\" with 10% bonus added to the original salary\n",
        "employees_with_bonus = employees_df.withColumn('SalaryAfterBonus', round(col('Salary') * 1.1))\n",
        "employees_with_bonus.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f13tKY60jhls"
      },
      "source": [
        "#### Task 9: Find Maximum Salary by Age\n",
        "\n",
        "Group the data by age and calculate the maximum salary for each age group. Display the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "KKiSrcHGjhls",
        "outputId": "ce63cd09-aa61-41fe-c92e-ca55438016e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------+\n",
            "|Age|max_Salary|\n",
            "+---+----------+\n",
            "| 29|     10000|\n",
            "| 30|      8000|\n",
            "| 34|      7800|\n",
            "| 28|     12008|\n",
            "| 35|      9000|\n",
            "| 31|      8200|\n",
            "| 27|     17000|\n",
            "| 26|      3600|\n",
            "| 38|      6000|\n",
            "| 33|     12008|\n",
            "| 32|     13000|\n",
            "| 36|      7900|\n",
            "| 37|     17000|\n",
            "| 39|     24000|\n",
            "+---+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import max\n",
        "\n",
        "# Group data by age and calculate the maximum salary for each age group\n",
        "max_sal_by_age = spark.sql(\"SELECT Age, MAX(Salary) as max_Salary FROM employees GROUP BY Age\")\n",
        "max_sal_by_age.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN7eYiQPjhls"
      },
      "source": [
        "#### Task 10: Self-Join on Employee Data\n",
        "\n",
        "Join the \"employees_df\" DataFrame with itself based on the \"Emp_No\" column. Display the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "OhpMswNwjhls",
        "outputId": "a0274fbe-9353-4f1f-e1fe-3888d566a5ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 83:======================>                                (30 + 10) / 75]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-----------+------+---+----------+-----------+------+---+----------+\n",
            "|Emp_No|   Emp_Name|Salary|Age|Department|   Emp_Name|Salary|Age|Department|\n",
            "+------+-----------+------+---+----------+-----------+------+---+----------+\n",
            "|   137|     Renske|  3600| 26| Marketing|     Renske|  3600| 26| Marketing|\n",
            "|   133|      Jason|  3300| 38|     Sales|      Jason|  3300| 38|     Sales|\n",
            "|   108|      Nancy| 12008| 28|     Sales|      Nancy| 12008| 28|     Sales|\n",
            "|   101|      Neena| 17000| 27|     Sales|      Neena| 17000| 27|     Sales|\n",
            "|   115|  Alexander|  3100| 29|   Finance|  Alexander|  3100| 29|   Finance|\n",
            "|   126|      Irene|  2700| 28|        HR|      Irene|  2700| 28|        HR|\n",
            "|   103|  Alexander|  9000| 39| Marketing|  Alexander|  9000| 39| Marketing|\n",
            "|   128|     Steven|  2200| 33|   Finance|     Steven|  2200| 33|   Finance|\n",
            "|   122|      Payam|  7900| 36|   Finance|      Payam|  7900| 36|   Finance|\n",
            "|   111|     Ismael|  7700| 32|        IT|     Ismael|  7700| 32|        IT|\n",
            "|   140|     Joshua|  2500| 29|   Finance|     Joshua|  2500| 29|   Finance|\n",
            "|   132|         TJ|  2100| 34|        IT|         TJ|  2100| 34|        IT|\n",
            "|   206|    William|  8300| 37|        IT|    William|  8300| 37|        IT|\n",
            "|   205|    Shelley| 12008| 33|   Finance|    Shelley| 12008| 33|   Finance|\n",
            "|   139|       John|  2700| 36|     Sales|       John|  2700| 36|     Sales|\n",
            "|   120|    Matthew|  8000| 30|        HR|    Matthew|  8000| 30|        HR|\n",
            "|   117|      Sigal|  2800| 33|     Sales|      Sigal|  2800| 33|     Sales|\n",
            "|   112|Jose Manuel|  7800| 34|        HR|Jose Manuel|  7800| 34|        HR|\n",
            "|   127|      James|  2400| 31|        HR|      James|  2400| 31|        HR|\n",
            "|   107|      Diana|  4200| 35|     Sales|      Diana|  4200| 35|     Sales|\n",
            "+------+-----------+------+---+----------+-----------+------+---+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Join the DataFrame with itself based on the \"Emp_No\" column\n",
        "joined_df = employees_df.join(employees_df, 'Emp_No', 'inner')\n",
        "joined_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGTv1b65jhls"
      },
      "source": [
        "#### Task 11: Calculate Average Employee Age\n",
        "\n",
        "Calculate the average age of employees using the built-in aggregation function. Display the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "e6QWVncRjhlt",
        "outputId": "9177a00c-5afc-4c8b-90e1-3afa314d1435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "|AverageAge|\n",
            "+----------+\n",
            "|     33.56|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate the average age of employees\n",
        "from pyspark.sql.functions import avg\n",
        "average_age_df = employees_df.agg(avg(\"Age\").alias(\"AverageAge\"))\n",
        "average_age_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8H6pnozjhlt"
      },
      "source": [
        "#### Task 12: Calculate Total Salary by Department\n",
        "\n",
        "Calculate the total salary for each department using the built-in aggregation function. Display the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "bI8Cb8Vajhlt",
        "outputId": "e3f15bb1-8ee7-41d5-9c5b-fea7ed5a17fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----------+\n",
            "|Department|TotalSalary|\n",
            "+----------+-----------+\n",
            "|     Sales|      71408|\n",
            "|        HR|      46700|\n",
            "|   Finance|      57308|\n",
            "| Marketing|      59700|\n",
            "|        IT|      74000|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate the total salary for each department. Hint - User GroupBy and Aggregate functions\n",
        "from pyspark.sql.functions import sum\n",
        "total_sal_df = employees_df.groupBy(\"Department\").agg(sum(\"Salary\").alias(\"TotalSalary\"))\n",
        "total_sal_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvdE5bJyjhlt"
      },
      "source": [
        "#### Task 13: Sort Data by Age and Salary\n",
        "\n",
        "Apply a transformation to sort the DataFrame by age in ascending order and then by salary in descending order. Display the sorted DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "cEKHRPcOjhlt",
        "outputId": "e670ed2d-644f-4bc9-c3d4-379de1369f0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+---------+------+---+----------+\n",
            "|Emp_No| Emp_Name|Salary|Age|Department|\n",
            "+------+---------+------+---+----------+\n",
            "|   137|   Renske|  3600| 26| Marketing|\n",
            "|   101|    Neena| 17000| 27|     Sales|\n",
            "|   114|      Den| 11000| 27|   Finance|\n",
            "|   108|    Nancy| 12008| 28|     Sales|\n",
            "|   130|    Mozhe|  2800| 28| Marketing|\n",
            "|   126|    Irene|  2700| 28|        HR|\n",
            "|   204|  Hermann| 10000| 29|   Finance|\n",
            "|   115|Alexander|  3100| 29|   Finance|\n",
            "|   134|  Michael|  2900| 29|     Sales|\n",
            "|   198|   Donald|  2600| 29|        IT|\n",
            "|   140|   Joshua|  2500| 29|   Finance|\n",
            "|   136|    Hazel|  2200| 29|        IT|\n",
            "|   120|  Matthew|  8000| 30|        HR|\n",
            "|   110|     John|  8200| 31| Marketing|\n",
            "|   127|    James|  2400| 31|        HR|\n",
            "|   201|  Michael| 13000| 32|        IT|\n",
            "|   111|   Ismael|  7700| 32|        IT|\n",
            "|   119|    Karen|  2500| 32|   Finance|\n",
            "|   205|  Shelley| 12008| 33|   Finance|\n",
            "|   124|    Kevin|  5800| 33| Marketing|\n",
            "+------+---------+------+---+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import asc, desc\n",
        "\n",
        "# Sort the DataFrame by age in ascending order and then by salary in descending order\n",
        "sorted_df = employees_df.orderBy(asc(\"Age\"), desc(\"Salary\"))\n",
        "sorted_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pJBP0Rfjhlt"
      },
      "source": [
        "#### Task 14: Count Employees in Each Department\n",
        "\n",
        "Calculate the number of employees in each department. Display the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "J2lmgAEGjhlt",
        "outputId": "31dedaa6-8879-4e33-e5c9-9b57832a21ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-------------+\n",
            "|Department|EmployeeCount|\n",
            "+----------+-------------+\n",
            "|     Sales|           13|\n",
            "|        HR|            8|\n",
            "|   Finance|           10|\n",
            "| Marketing|            9|\n",
            "|        IT|           10|\n",
            "+----------+-------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "# Calculate the number of employees in each department\n",
        "emp_count = employees_df.groupBy(\"Department\").agg(count(\"Emp_No\").alias(\"EmployeeCount\"))\n",
        "emp_count.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1mZ7Sdpjhl1"
      },
      "source": [
        "#### Task 15: Filter Employees with the letter o in the Name\n",
        "\n",
        "Apply a filter to select records where the employee's name contains the letter `'o'`. Display the filtered DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VjanXu_jhl1",
        "outputId": "705b5aff-e997-4d1c-dd18-b3c80c94b7fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+------+---+----------+\n",
            "|Emp_No|   Emp_Name|Salary|Age|Department|\n",
            "+------+-----------+------+---+----------+\n",
            "|   198|     Donald|  2600| 29|        IT|\n",
            "|   199|    Douglas|  2600| 34|     Sales|\n",
            "|   110|       John|  8200| 31| Marketing|\n",
            "|   112|Jose Manuel|  7800| 34|        HR|\n",
            "|   130|      Mozhe|  2800| 28| Marketing|\n",
            "|   133|      Jason|  3300| 38|     Sales|\n",
            "|   139|       John|  2700| 36|     Sales|\n",
            "|   140|     Joshua|  2500| 29|   Finance|\n",
            "+------+-----------+------+---+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Apply a filter to select records where the employee's name contains the letter 'o'\n",
        "name_filter_df = employees_df.filter(col(\"Emp_Name\").like(\"%o%\"))\n",
        "name_filter_df.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "conda-env-python-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}